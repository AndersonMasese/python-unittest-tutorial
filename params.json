{"note":"Don't delete this file! It's used internally to help with page regeneration.","name":"Python-unittest-tutorial","tagline":"python `unittest` tutorial (unittest PyMOTW for Python 3.3)","body":"# Python Unit Testing Tutorial\r\n\r\nThis is an update to Doug Hellman's excellent PyMOTW article found here:\r\n\r\n* [PyMOTW - unittest (2007)](http://www.doughellmann.com/PyMOTW/unittest/index.html)\r\n\r\nThe code and examples here have been updated by [Corey Goldberg](https://github.com/cgoldberg) to reflect Python 3.3.\r\n\r\nfurther reading:\r\n\r\n* [unittest - Python Standard Library 3.3 Documentation](http://docs.python.org/3.3/library/unittest.html)\r\n  \r\n----\r\n\r\n## unittest - Automated testing framework\r\n\r\nPython's `unittest` module, sometimes referred to as 'PyUnit', is based on the XUnit framework design by Kent Beck and Erich Gamma. The same pattern is repeated in many other languages, including C, Perl, Java, and Smalltalk. The framework implemented by `unittest` supports fixtures, test suites, and a test runner to enable automated testing for your code.\r\n\r\n## Basic Test Structure\r\n\r\nTests, as defined by unittest, have two parts: code to manage test \"fixtures\", and the test itself. Individual tests are created by subclassing `TestCase` and overriding or adding appropriate methods. For example,\r\n\r\n```python\r\nimport unittest\r\n\r\nclass SimplisticTest(unittest.TestCase):\r\n\r\n    def test(self):\r\n        self.assertTrue(True)\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n```\r\n\r\nIn this case, the `SimplisticTest` has a single `test()` method, which would fail if True is ever False.\r\n\r\n## Running Tests\r\n\r\nThe easiest way to run unittest tests is to include:\r\n\r\n```python\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n```\r\n\r\nat the bottom of each test file, then simply run the script directly from the command line:\r\n\r\n```\r\n$ python3 test_simple.py\r\n\r\n.\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.000s\r\n\r\nOK\r\n```\r\n\r\nThis abbreviated output includes the amount of time the tests took, along with a status indicator for each test (the \".\" on the first line of output means that a test passed). For more detailed test results, include the `-v` option:\r\n\r\n```\r\n$ python3 test_simple.py -v\r\n\r\ntest (__main__.SimplisticTest) ... ok\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.000s\r\n\r\nOK\r\n```\r\n\r\n## Test Outcomes\r\n\r\nTests have 3 possible outcomes:\r\n\r\n```\r\nok\r\n```\r\n\r\nThe test passes.\r\n\r\n```\r\nFAIL\r\n```\r\n\r\nThe test does not pass, and raises an `AssertionError` exception.\r\n\r\n```\r\nERROR\r\n```\r\n\r\nThe test raises an exception other than `AssertionError`.\r\n\r\nThere is no explicit way to cause a test to \"pass\", so a test's status depends on the presence (or absence) of an exception.\r\n\r\n```python\r\nimport unittest\r\n\r\nclass OutcomesTest(unittest.TestCase):\r\n\r\n    def test_pass(self):\r\n        self.assertTrue(True)\r\n\r\n    def test_fail(self):\r\n        self.assertTrue(False)\r\n\r\n    def test_error(self):\r\n        raise RuntimeError('Test error!')\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n```\r\n\r\nWhen a test fails or generates an error, the traceback is included in the output.\r\n\r\n```\r\n$ python3 test_outcomes.py\r\n\r\n\r\nEF.\r\n======================================================================\r\nERROR: test_error (__main__.OutcomesTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_outcomes.py\", line 13, in test_error\r\n    raise RuntimeError('Test error!')\r\nRuntimeError: Test error!\r\n\r\n======================================================================\r\nFAIL: test_fail (__main__.OutcomesTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_outcomes.py\", line 9, in test_fail\r\n    self.assertTrue(False)\r\nAssertionError: False is not true\r\n\r\n----------------------------------------------------------------------\r\nRan 3 tests in 0.000s\r\n\r\nFAILED (failures=1, errors=1)\r\n```\r\n\r\nIn the example above, `test_fail()` fails and the traceback shows the line with the failure code. It is up to the person reading the test output to look at the code to figure out the semantic meaning of the failed test, though. To make it easier to understand the nature of a test failure, the `assert*()` methods all accept an argument `msg`, which can be used to produce a more detailed error message.\r\n\r\n```python\r\nimport unittest\r\n\r\nclass FailureMessageTest(unittest.TestCase):\r\n\r\n    def test_fail(self):\r\n        self.assertTrue(False, 'failure message goes here')\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n```\r\n\r\n```\r\n$ python3 test_failwithmessage.py -v\r\n\r\ntest_fail (__main__.FailureMessageTest) ... FAIL\r\n\r\n======================================================================\r\nFAIL: test_fail (__main__.FailureMessageTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_failwithmessage.py\", line 6, in test_fail\r\n    self.assertTrue(False, 'failure message goes here')\r\nAssertionError: failure message goes here\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.000s\r\n\r\nFAILED (failures=1)\r\n```\r\n\r\n## Asserting Truth\r\n\r\nMost tests assert the truth of some condition. There are a few different ways to write truth-checking tests, depending on the perspective of the test author and the desired outcome of the code being tested. If the code produces a value which can be evaluated as true, the method `assertTrue()` should be used. If the code produces a false value, the method `assertFalse()` makes more sense.\r\n\r\n```python\r\nimport unittest\r\n\r\nclass TruthTest(unittest.TestCase):\r\n\r\n    def test_assert_true(self):\r\n        self.assertTrue(True)\r\n\r\n    def test_assert_false(self):\r\n        self.assertFalse(False)\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n```\r\n\r\n```\r\n$ python3 test_truth.py -v\r\n\r\ntest_assert_false (__main__.TruthTest) ... ok\r\ntest_assert_true (__main__.TruthTest) ... ok\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.000s\r\n\r\nOK\r\n```\r\n\r\n## Assertion Methods\r\n\r\nThe `TestCase` class provides a number of methods to check for and report failures:\r\n\r\n* [`TestCase` class](http://docs.python.org/3.3/library/unittest.html#unittest.TestCase)\r\n* [`assert*` methods](http://docs.python.org/3.3/library/unittest.html#assert-methods)\r\n\r\n### Common Assertions\r\n\r\n<table>\r\n  <tr><th>Method</th></tr>\r\n  <tr><td><code>assertTrue(x, msg=None)</code></td></tr>\r\n  <tr><td><code>assertFalse(x, msg=None)</code></td></tr>\r\n  <tr><td><code>assertIsNone(x, msg=None)</code></td></tr>\r\n  <tr><td><code>assertIsNotNone(x, msg=None)</code></td></tr>\r\n  <tr><td><code>assertEqual(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertNotEqual(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertIs(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertIsNot(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertIn(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertNotIn(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertIsInstance(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertNotIsInstance(a, b, msg=None)</code></td></tr>\r\n</table>\r\n\r\n### Other Assertions\r\n\r\n<table>\r\n  <tr><th>Method</th></tr>\r\n  <tr><td><code>assertAlmostEqual(a, b, places=7, msg=None, delta=None)</code></td></tr>\r\n  <tr><td><code>assertNotAlmostEqual(a, b, places=7, msg=None, delta=None)</code></td></tr>\r\n  <tr><td><code>assertGreater(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertGreaterEqual(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertLess(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertLessEqual(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertRegex(text, regexp, msg=None)</code></td></tr>\r\n  <tr><td><code>assertNotRegex(text, regexp, msg=None)</code></td></tr>\r\n  <tr><td><code>assertCountEqual(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertMultiLineEqual(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertSequenceEqual(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertListEqual(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertTupleEqual(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertSetEqual(a, b, msg=None)</code></td></tr>\r\n  <tr><td><code>assertDictEqual(a, b, msg=None)</code></td></tr>\r\n</table>\r\n\r\n### Failure Messages\r\n\r\nThese assertions are handy, since the values being compared appear in the failure message when a test fails.\r\n\r\n```python\r\nimport unittest\r\n\r\nclass InequalityTest(unittest.TestCase):\r\n\r\n    def testEqual(self):\r\n        self.assertNotEqual(1, 3-2)\r\n\r\n    def testNotEqual(self):\r\n        self.assertEqual(2, 3-2)\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n```\r\n\r\nAnd when these tests are run:\r\n\r\n```\r\n$ python3 test_notequal.py -v\r\n\r\ntestEqual (__main__.InequalityTest) ... FAIL\r\ntestNotEqual (__main__.InequalityTest) ... FAIL\r\n\r\n======================================================================\r\nFAIL: test_equal (__main__.InequalityTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_notequal.py\", line 7, in test_equal\r\n    self.assertNotEqual(1, 3-2)\r\nAssertionError: 1 == 1\r\n\r\n======================================================================\r\nFAIL: test_not_equal (__main__.InequalityTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_notequal.py\", line 10, in test_not_equal\r\n    self.assertEqual(2, 3-2)\r\nAssertionError: 2 != 1\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.000s\r\n\r\nFAILED (failures=2)\r\n```\r\n\r\nAll the assert methods above accept a `msg` argument that, if specified, is used as the error message on failure.\r\n\r\n## Testing for Exceptions (and Warnings)\r\n\r\nThe `TestCase` class provides methods to check for expected exceptions:\r\n\r\n* [`TestCase` class](http://docs.python.org/3.3/library/unittest.html#unittest.TestCase)\r\n* [`assert*` methods](http://docs.python.org/3.3/library/unittest.html#assert-methods)\r\n\r\n<table>\r\n  <tr><th>Method</th></tr>\r\n  <tr><td><code>assertRaises(exception)</code></td></tr>\r\n  <tr><td><code>assertRaisesRegex(exception, regexp)</code></td></tr>\r\n  <tr><td><code>assertWarns(warn, fun, *args, **kwds)</code></td></tr>\r\n  <tr><td><code>assertWarnsRegex(warn, fun, *args, **kwds)</code></td></tr>\r\n</table>\r\n\r\nAs previously mentioned, if a test raises an exception other than `AssertionError` it is treated as an error. This is very useful for uncovering mistakes while you are modifying code which has existing test coverage. There are circumstances, however, in which you want the test to verify that some code does produce an exception. For example, if an invalid value is given to an attribute of an object. In such cases, `assertRaises()` makes the code more clear than trapping the exception yourself. Compare these two tests:\r\n\r\n```python\r\nimport unittest\r\n\r\ndef raises_error(*args, **kwds):\r\n    raise ValueError('Invalid value: %s%s' % (args, kwds))\r\n\r\nclass ExceptionTest(unittest.TestCase):\r\n\r\n    def test_trap_locally(self):\r\n        try:\r\n            raises_error('a', b='c')\r\n        except ValueError:\r\n            pass\r\n        else:\r\n            self.fail('Did not see ValueError')\r\n\r\n    def test_assert_raises(self):\r\n        self.assertRaises(ValueError, raises_error, 'a', b='c')\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n```\r\n\r\nThe results for both are the same, but the second test using `assertRaises()` is more succinct.\r\n\r\n```\r\n$ python3 test_exception.py -v\r\n\r\ntest_assert_raises (__main__.ExceptionTest) ... ok\r\ntest_trap_locally (__main__.ExceptionTest) ... ok\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.000s\r\n\r\nOK\r\n```\r\n\r\n## Test Fixtures\r\n\r\nFixtures are resources needed by a test. For example, if you are writing several tests for the same class, those tests all need an instance of that class to use for testing. Other test fixtures include database connections and temporary files (many people would argue that using external resources makes such tests not \"unit\" tests, but they are still tests and still useful). `TestCase` includes a special hook to configure and clean up any fixtures needed by your tests. To configure the fixtures, override `setUp()`. To clean up, override `tearDown()`.\r\n\r\n```python\r\nimport unittest\r\n\r\nclass FixturesTest(unittest.TestCase):\r\n\r\n    def setUp(self):\r\n        print('In setUp()')\r\n        self.fixture = range(1, 10)\r\n\r\n    def tearDown(self):\r\n        print('In tearDown()')\r\n        del self.fixture\r\n\r\n    def test(self):\r\n        print('in test()')\r\n        self.assertEqual(self.fixture, range(1, 10))\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n```\r\n\r\nWhen this sample test is run, you can see the order of execution of the fixture and test methods:\r\n\r\n```\r\n$ python3 test_fixtures.py\r\n\r\n.\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.000s\r\n\r\nOK\r\nIn setUp()\r\nin test()\r\nIn tearDown()\r\n```\r\n\r\n## Test Suites\r\n\r\nThe standard library documentation describes how to organize test suites manually. I generally do not use test suites directly, because I prefer to build the suites automatically (these are automated tests, after all). Automating the construction of test suites is especially useful for large code bases, in which related tests are not all in the same place. Tools such as nose make it easier to manage tests when they are spread over multiple files and directories.\r\n","google":""}